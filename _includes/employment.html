<section class="thirteen columns">
    <h1>Employment</h1>
    <article>
        <header>
            <h1>Full Stack Web Developer</h1>
            <span><address>Exafluence, India</address><time>June 2016 to Aug 2018</time></span>
        </header>
        <p>
                EXAFLUNCE is technology solution company focused on solving complex problems leveraging Big Data and Analytics 
                utilizing new technologies and frameworks. Our accelerators and solutions use advanced technologies in IOT, 
                Big Data and Machine learning to provide actionable insights. We develop expedited solutions built on micro 
                services architecture for financial services
                 and Health care verticals    

        </p>
    </article>
    <h3>Projects</h3>
    <article>
        <header>
            <h1>AnalyticsConnect - Full stack - Django</h1>
            <span> <time>June 2016 to Jan 2017</time></span>
        </header>
        <p>AnalyticsConnect is online web based job portal, only dedicated for big-data technology people, and also have virtual COE to support the team members working on client space. The projects at Analytics Connect focused on developing a Modern website using popularly used python , web framework Django and MongoDB as database </p>
        <i class="fa fa-globe" style="color: blue" aria-hidden="true"></i> <a style="color: blueviolet" href="http://analyticsconnect.info">Analytics Connect Site</a>
        <br><br>
            <p> 
                
<h4>Responsibilities</h4>
                • Implemented resume-parser using textract, accepting pdf,doc,docx formats <br>

                • Designed a real-time chat module to support team communication.  <br>
                • Used Django web framework and MongoDB 3.4 as Database engine <br>
                • Implemented django-bootstrap3 for frontend <br>
                • Deployed both the Django and MongoDB in Go-daddy VPS hosting ,and solved few bugs appeared <br>
                • Can be able to successfully migrate the Django-models into database schema    <br>


            </p>
    </article>
    <article>
        <header>
            <h1>AnalyticsConnect - Spark Developer</h1>
            <span> <time>Feb 2017 to Apr 2017</time></span>
        </header>
        <p>AnalyticsConnect is online web based job portal, only dedicated for big-data technology people, and also have virtual COE to support the team members working on client space. The projects at Analytics Connect focused on developing a Modern website using popularly used python , web framework Django and MongoDB as database </p>
        <i class="fa fa-globe" style="color: blue" aria-hidden="true"></i> <a style="color: blueviolet" href="http://analyticsconnect.info">Analytics Connect Site</a>
        <br><br>
            <p> 
                
<h4>Responsibilities</h4>

                • Worked towards the designing and deployment of Spark, Mongo DB, java spark-mongo drivers <br>

                • Able to launch the Spark-jobs from Django application api using `spark-jobserver`   <br>
                • Developed Java Code using spark RDD, and applied regex-match to grep the required information operation on each partition. Mapped users Id with respect to generated result and saves to the MongoDB.    <br>
                • Experience in the installation & configuration of Apache Spark on GoDaddy VPS.  <br>
                • Deployed project in Spark-standalone mode.<br>
                


            </p>
    </article>

    <article>
        <header>
            <h1>LabLinks BioTech</h1>
            <span> <time>May 2017 to Sep 2017</time></span>
        </header>
        <p> 
                This manufacturer is a supplier and exporter of specialized equipment for biotechnology laboratories and industry such as bio-reactors, custom built roller bottle equipment with built in incubators, disposable bio-reactor systems of patented design, blotting manifolds, and filtration systems. The projects at LABLINKS BIOTECH focused on developing a Dynamic website, so that obtained data from database will be used to perform the analysis or business logic on it using big-data technologies

        </p>
        <br>
        <h4>Responsibilities</h4>
        
        • Developed User Interface for the project using JavaScript, JSP, HTML, CSS. <br>
        • Worked on MongoDB for implementing CRUD operations and authentication system <br>
        • Involved in bug fixing during the Unit testing, Integrated System testing and User acceptance testing. <br>
        • Able to deploy project in AWS instance and worked with client onsite. <br>

    </article>

    <article>
        <header>
            <h1>Authorization of Pended Claims </h1>
            <span> <time>Oct 2017 to Mar 2018</time></span>
        </header>
        <p>This project provides the details of Pended claims details in Health care in USA, for the particular claim status, shows the entire insurance amount and due amount.   </p><br>

        <h4>Responsibilities</h4> 

        •Used Spark 2.2 with Java 8 to get data from Oracle DB.<br>

        •Using OracleJDBC driver connected the spark application to the oracle database url<br>

        •Implemented Mappartion over Map while writing data to the database.<br>

        •Created UDF (User Defined Function) for converting the zip code, date-format to required<br>

        •Used complex Spark-SQL JOINS queries on dataset to generate the report.<br>

        •Fine-tuned the partition level to avoid uneven partitions.<br>

        •Preferred spark-sql built-in function to UDF because of performance optimization•Created Schema externally for the dataset using StructType.<br>

    </article>

    <article>
        <header>
            <h1>ExaHealth - Healthcare vertical</h1>
            <span> <time>Apr 2018 to Aug 2018</time></span>
        </header>
        <p>NpiAlerts, is the module dedicated to track the changes of NPIs records from CMS database. This provides the updated NPIs records for every 1 hours span, where as CMS will provides weekly in weekly updates section. It also provides history of changes that a particular NPI record had gone through</p><br>

        <h4>Responsibilities</h4>   

        • Used Java FutureTask to call the CMS API endpoint to get NPI records in parallel, hence reduced the  download time of 120 million records (240 GB) from days to 53 minutes. <br>
        • Spark used to load the individual downloaded JSON files to a dataset and applied business logic on it.<br>
        • Used HDFS for storage and YARN for the cluster manager.<br>
        • Able to setup multi-node Hadoop cluster with 3 (t.xlarge)AWS instances. <br>
        • Applied UDFs (User defined Function) to format on particular columns to meet the required logic.<br>
        • Used spark persistmethod on dataset to perform fast analytics in multiple iterations.<br>
        • Provided schema externally for 350 columns instead of inferSchema option on JSON, optimized     performance to 2x.<br>
        • Monitored Spark-UI for any straggler task and slated keys in-case while performing joinoperations     <br>
        • Made an application in two modes, on demand and micro-batch mode. On-Demand mode lets thespark job to execute  by sending a request from web-app using Spark-Job-Server. The micro-batch mode will    execute in continuous fashion<br>
        • Using mongodb as the database, stored the updated set of NPI records for every iteration<br>
        • UI/UX accomplished with JSP, CSS as frontend and Java as a backend.      

    </article>
    
  
   

</section>
